% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/make_counts.R
\name{make_counts}
\alias{make_counts}
\title{Make a count table}
\usage{
make_counts(
  input,
  trimming = NULL,
  threads = 1,
  plot = TRUE,
  parse = "default_illumina",
  evidence = c(experiment = 2, sample = 1),
  save_temp = FALSE
)
}
\arguments{
\item{input}{A path to a directory containing input fastq-files. The script
will recursively search this directory for the .fastq|.fastq.gz extension.}

\item{trimming}{Character indicating what type of trimming tool that should
be used. If \code{trimming="seqpac"}, fastq files will be sent to the
\code{\link{make_trim}} function in seqpac, while if
\code{trimming="cutadapt"} fastq files will be sent to the
\code{\link{make_cutadapt}} function. Note that \code{trimming="seqpac"}
runs independently of external software, while \code{trimming="cutadapt"}
is dependent on an externally installed version of cutadapt and
fastq_quality_filter. Trimmed fastq files are stored temporarily in the
systems default temporary folder. Please, run \code{\link{make_trim}} and
\code{\link{make_cutadapt}} seperately for perminant storage options, or
set \code{save_temp=TRUE} to avoid that \code{make_counts} will delete all
temporary files.  As default trimming=NULL, which indicates that input
fastq files has already been trimmed.}

\item{threads}{Integer stating the number of parallell jobs. Note, that
reading multiple fastq files drains memory fast, using up to 10Gb per fastq
file. To avoid crashing the system due to memory shortage, make sure that
each thread on the machine have at least 10 Gb of memory availabe, unless
your fastq files are very small. Use \code{parallel::detectcores()} to see
available threads on the machine.}

\item{plot}{Logical whether evidence plots should be printed and saved in the
returned list (default=TRUE).}

\item{parse}{Character strings defining the command that should be parsed to
\code{\link{make_trim}} or \code{\link{make_cutadapt}}. This will allow
you to customize your trimming according to 3' adaptor sequence and
platform standards etc. Please see examples below and the manuals for
\code{\link{make_trim}} and \code{\link{make_cutadapt}} for more details.
For convenience, \code{parse} also have two default mode for sRNA trimming,
using Illumina and New England Biotype (neb) type small RNA adaptors.
\code{make_counts} will automatically print the exact setting for each
default mode. Briefly, both modes involves polyG (NextSeq/NovaSeq) trimming
and 3' adaptor trimming, with a 0.1 tolerance for mismatch/indels. If
parse="default_illumina", then the "TGGAATTCTCGGGTGCCAAGGAACTCCAGTCAC" 3'
adaptor is trimmed and untrimmed sequences are removed. If
parse="default_neb", then "AGATCGGAAGAGCACACGTCTGAACTCCA" is trimmed and
untrimmed sequences are removed. Removing untrimmed sequences is
recommended for sRNA sequencing.}

\item{evidence}{Character vector with two inputs named 'experiment' and
'sample' that controls the low-level evidence filter. Users may already at
this point markly reduce the level of noise in the counts table by
specifying the number of independent evidence that a specific sequence must
have to be included. As default,
\code{evidence=c(experiment=2, sample=1)} will include all sequences that
have >=1 count in at least 2 independent fastq files. Thus 'experiment'
controls the number of independent fastq evidence needed across the whole
experiment. Note, however, that 'sample' does not control the number of
counts needed in each sample. The evidence filter will always use >=1 count
in X number of fastq files. Instead 'sample' controls when a sequence
should be included despite not reaching the 'experiment' threshold. Thus if
\code{evidence=c(experiment=2, sample=10)}, sequences that reach 10 counts
in a single sample will also be included. If evidence=NULL all unique
sequences will be saved. See 'examples' below for more examples.}

\item{save_temp}{Logical whether temporary files (including trimmed fastq
files) should be saved or not. Note, the function will print the path to
the temprorary folder in the console.}
}
\value{
A list containing three objects:
  1. counts (data frame) = count table. 
  2. progress_report = progress report from trimming and evidence filter.
  3. evidence_plots = bar graphs showing the impact of evidence filter.
}
\description{
\code{make_counts} uses parallel processing to generate a count table.
}
\details{
Given a paths to fastq this function performs low-level evidence filtering,
generates a counts table of sequences passing the filter and plots summary
statistics.
}
\examples{

\dontrun{

 library(seqpac)
  
############################################################ 
### Seqpac fastq trimming with the make_trim function 
### using default settings for NEBNext small RNA adaptor 

input = system.file("extdata", package = "seqpac", mustWork = TRUE)

counts  <- make_counts(input, threads=1, parse="default_neb",
                       type="fastq", trimming="seqpac", plot=TRUE,
                       evidence=c(experiment=2, sample=1))     
    
     
############################################################      
### Seqpac trimming using the make_trim function
### using user settings parsed to make_trim
 
# Make a parse list (see ?make_trim):  
parse = list(adapt_3_set=c(type="hard_save", min=10, mismatch=0.1),
             adapt_3="AGATCGGAAGAGCACACGTCTGAACTCCAGTCACTA",
             polyG=c(type="hard_trim", min=10, mismatch=0.1),
             seq_range=c(min=14, max=70),
             quality=c(threshold=20, percent=0.8))
               
counts  <-  make_counts(input, threads=1,
                        type="fastq", trimming="seqpac",
                        parse=parse, 
                        evidence=c(experiment=2, sample=1))           
  
  
############################################################      
### Seqpac trimming using the make_cutadapt function
### (Important: Needs an external installations of cutadapt 
###  and fastq_quality_filter) 

 input <- system.file("extdata", package = "seqpac", mustWork = TRUE)
 
 Parse for make_cutadapt is a list of 2 character string expressions.
 The first is parsed to cutadapt and the other to fastq_quality_filter 
 For parallel processes '-j 1' is recommended since seqpac will   
 parallelize across samples and not within. Run system("cutadapt -h") and 
 system("fastq_quality_filter -h") for more options.
 
 cut_prse <- paste0("-j 1 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCACAT", 
                    " --discard-untrimmed --nextseq-trim=20",
                    " -O 10 -m 14 -M 70")
 
 parse = list(cutadapt = cut_prse,
              fastq_quality_filter = "-q 20 -p 80")
 
 counts  <-  make_counts(input, threads=1,
                        type="fastq", trimming="cutadapt",
                        parse=parse, 
                        evidence=c(experiment=2, sample=1))   
 
 
 ## 2 evidence over two indepenent samples, saving single sample 
 ## sequences reaching 10 counts
 test <- make_counts(input=input,  type="fastq", trimming="seqpac", 
                     parse="default_neb",  
                     evidence=c(experiment=2, sample=10))
 extras <- apply(test$counts, 1, function(x){sum(!x==0)})
 test$counts[extras==1,]  28 single sample sequences reached 10 counts
 
 
 ## 2 evidence over two indepenent samples, saving single sample 
 ## sequences reaching 3 counts 
 test <- make_counts(input=input,  type="fastq", trimming="seqpac", 
                     parse="default_neb",  
                     evidence=c(experiment=2, sample=3))
 extras <- apply(test$counts, 1, function(x){sum(!x==0)})
 test$counts[extras==1,] 1319 single sample sequences reached 3 counts
 
 }
 
 
}
\seealso{
\url{https://github.com/Danis102} for updates on the current
  package.

Other PAC generation: 
\code{\link{PAC_check}()},
\code{\link{make_PAC}()},
\code{\link{make_cutadapt}()},
\code{\link{make_pheno}()},
\code{\link{make_trim}()},
\code{\link{progress_report}()}
}
\concept{PAC generation}
