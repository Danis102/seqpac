% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/make_counts.R
\name{make_counts}
\alias{make_counts}
\title{Make a count table}
\usage{
make_counts(
  input,
  trimming = NULL,
  threads = 1,
  plot = TRUE,
  parse = "default_illumina",
  evidence = c(experiment = 2, sample = 1),
  save_temp = FALSE
)
}
\arguments{
\item{input}{A path to a directory containing input fastq-files. The script
will recursively search this directory for the .fastq|.fastq.gz extension.}

\item{trimming}{Character indicating what type of trimming tool that should
be used. If \code{trimming="seqpac"}, fastq files will be sent to the
\code{\link{make_trim}} function in seqpac, while if
\code{trimming="cutadapt"} fastq files will be sent to the
\code{\link{make_cutadapt}} function. Note that \code{trimming="seqpac"}
runs independently of external software, while \code{trimming="cutadapt"}
is dependent on an externally installed version of cutadapt and
fastq_quality_filter. Trimmed fastq files are stored temporarily in the
systems default temporary folder. Please, run \code{\link{make_trim}} and
\code{\link{make_cutadapt}} seperately for perminant storage options, or
set \code{save_temp=TRUE} to avoid that \code{make_counts} will delete all
temporary files.  As default trimming=NULL, which indicates that input
fastq files has already been trimmed.}

\item{threads}{Integer stating the number of parallell jobs. Note, that
reading multiple fastq files drains memory fast, using up to 10Gb per fastq
file. To avoid crashing the system due to memory shortage, make sure that
each thread on the machine have at least 10 Gb of memory availabe, unless
your fastq files are very small. Use \code{parallel::detectcores()} to see
available threads on the machine.}

\item{plot}{Logical whether evidence plots should be printed and saved in the
returned list (default=TRUE).}

\item{parse}{Character strings defining the command that should be parsed to
\code{\link{make_trim}} or \code{\link{make_cutadapt}}. This will allow
you to customize your trimming according to 3' adaptor sequence and
platform standards etc. Please see examples below and the manuals for
\code{\link{make_trim}} and \code{\link{make_cutadapt}} for more details.
For convenience, \code{parse} also have two default mode for sRNA trimming,
using Illumina and New England Biotype (neb) type small RNA adaptors.
\code{make_counts} will automatically print the exact setting for each
default mode. Briefly, both modes involves polyG (NextSeq/NovaSeq) trimming
and 3' adaptor trimming, with a 0.1 tolerance for mismatch/indels. If
parse="default_illumina", then the "TGGAATTCTCGGGTGCCAAGGAACTCCAGTCAC" 3'
adaptor is trimmed and untrimmed sequences are removed. If
parse="default_neb", then "AGATCGGAAGAGCACACGTCTGAACTCCA" is trimmed and
untrimmed sequences are removed. Removing untrimmed sequences is
recommended for sRNA sequencing.}

\item{evidence}{Character vector with two inputs named 'experiment' and
'sample' that controls the low-level evidence filter. Users may already at
this point markly reduce the level of noise in the counts table by
specifying the number of independent evidence that a specific sequence must
have to be included. As default,
\code{evidence=c(experiment=2, sample=1)} will include all sequences that
have >=1 count in at least 2 independent fastq files. Thus 'experiment'
controls the number of independent fastq evidence needed across the whole
experiment. Note, however, that 'sample' does not control the number of
counts needed in each sample. The evidence filter will always use >=1 count
in X number of fastq files. Instead 'sample' controls when a sequence
should be included despite not reaching the 'experiment' threshold. Thus if
\code{evidence=c(experiment=2, sample=10)}, sequences that reach 10 counts
in a single sample will also be included. If evidence=NULL all unique
sequences will be saved. See 'examples' below for more examples.}

\item{save_temp}{Logical whether temporary files (including trimmed fastq
files) should be saved or not. Note, the function will print the path to
the temprorary folder in the console.}
}
\value{
A list containing three objects:

  1. counts (data frame) = count table. 
  
  2. progress_report = progress report from trimming and evidence filter.
  
  3. evidence_plots = bar graphs showing the impact of evidence filter.
}
\description{
\code{make_counts} uses parallel processing to generate a count table.
}
\details{
Given a paths to fastq this function performs low-level evidence filtering,
generates a counts table of sequences passing the filter and plots summary
statistics.
}
\examples{

  
############################################################ 
### Seqpac fastq trimming with the make_counts function 
### using default settings for NEBNext small RNA adaptor 

# First generate some smallRNA fastq.
# Only one untrimmed fastq comes with seqpac
# Thus, we need to randomly sample that one using the ShortRead-package
 
sys_path = system.file("extdata", package = "seqpac", mustWork = TRUE)
fq <- list.files(path = sys_path, pattern = "fastq", all.files = FALSE,
                full.names = TRUE)

closeAllConnections()

sampler <- ShortRead::FastqSampler(fq, 10000)
set.seed(123)
fqs <- list(fq1=ShortRead::yield(sampler),
           fq2=ShortRead::yield(sampler),
           fq3=ShortRead::yield(sampler))
           

# Now generate a temp folder where we can store the fastq files

input <- paste0(tempdir(), "/seqpac_temp")
dir.create(input, showWarnings=FALSE)

# And then write the random fastq to the temp folder
for (i in 1:length(fqs)){
 input_file <- file.path(input, paste0(names(fqs)[i], ".fastq.gz"))
 ShortRead::writeFastq(fqs[[i]], input_file, mode="w", 
                       full=FALSE, compress=TRUE)
}

# Now we can run make_counts
# Notice that make_counts will generate another temp folder, that will 
# be emptied on finalization. By setting save_temp=TRUE you may save the 
# content.
 
counts  <- make_counts(input, threads=2, parse="default_neb",
                       trimming="seqpac", plot=TRUE,
                       evidence=c(experiment=2, sample=1))

head(counts$counts)
head(counts$progress_report)
cowplot::plot_grid(plotlist=counts$evidence_plots, ncol=1, nrow=2)

# Notice that that there are fewer unique sequences than number of reads 
# passing the filter. In normal, large, fastq we keep 95-99\% of all reads 
# while filtering out 30-40\% of the sequences that only appear in 1 fastq 
# Thus, the evidence filter may gain you performance in later analysis by 
# avoiding nonsense sequences. 
     
############################################################      
### Parse your own trimming setting to make_trim using the
### make_counts function
### How to make a parse list see ?make_trim.
  
parse = list(adapt_3_set=c(type="hard_save", min=10, mismatch=0.1),
             adapt_3="AGATCGGAAGAGCACACGTCTGAACTCCAGTCACTA",
             polyG=c(type="hard_trim", min=10, mismatch=0.1),
             seq_range=c(min=14, max=70),
             quality=c(threshold=20, percent=0.8),
             check_mem =FALSE)
             

counts  <-  make_counts(input, threads=2,
                        trimming="seqpac",
                        parse=parse, plot=FALSE, 
                        evidence=c(experiment=2, sample=1))
  
  
  
############################################################      
### You may also use the make_cutadapt function for trimming.
### (Important: This needs an external installations of cutadapt 
###  and fastq_quality_filter). See ?make_cutadapt for details. 

 
# Clean up temp
closeAllConnections()
fls_temp  <- list.files(tempdir(), recursive=TRUE, full.names = TRUE)
file.remove(fls_temp, showWarnings=FALSE)


}
\seealso{
\url{https://github.com/Danis102} for updates on the current
  package.

Other PAC generation: 
\code{\link{PAC_check}()},
\code{\link{make_PAC}()},
\code{\link{make_cutadapt}()},
\code{\link{make_pheno}()},
\code{\link{make_trim}()},
\code{\link{merge_lanes}()}
}
\concept{PAC generation}
